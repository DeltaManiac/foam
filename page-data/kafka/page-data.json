{"componentChunkName":"component---node-modules-gatsby-theme-garden-src-templates-local-file-js","path":"/kafka","result":{"data":{"file":{"childMdx":{"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsx mdx */\nvar _frontmatter = {};\n\nvar makeShortcode = function makeShortcode(name) {\n  return function MDXDefaultShortcode(props) {\n    console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\");\n    return mdx(\"div\", props);\n  };\n};\n\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"apache-kafka\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Apache Kafka\"), mdx(\"ul\", null, mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"p\", {\n    \"parentName\": \"li\"\n  }, \"pub-sub messaging system\")), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"p\", {\n    \"parentName\": \"li\"\n  }, \"basic unit --> message =>\", mdx(\"inlineCode\", {\n    \"parentName\": \"p\"\n  }, \"{ Body :[u8], Key:[u8]}\"))), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"p\", {\n    \"parentName\": \"li\"\n  }, \"writes are mainly batch operations => \", mdx(\"inlineCode\", {\n    \"parentName\": \"p\"\n  }, \"Vec<Messages>\"))), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"p\", {\n    \"parentName\": \"li\"\n  }, \"does not track acknowledgments from consumers but allows consumers to use Kafka to track their position (offset) in each partition.\")), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"p\", {\n    \"parentName\": \"li\"\n  }, \"Schema --> governs structure of message\"), mdx(\"ul\", {\n    \"parentName\": \"li\"\n  }, mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"eg JSON/XML/\", mdx(\"a\", {\n    \"href\": \"https://avro.apache.org/docs/current/\",\n    \"parentName\": \"li\"\n  }, \"Apache Avro\")))), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"p\", {\n    \"parentName\": \"li\"\n  }, \"Topics --> collection of messages \\\"about something same\\\"\")), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"p\", {\n    \"parentName\": \"li\"\n  }, \"Topics made of n partitions => Append only queue-like\"), mdx(\"p\", {\n    \"parentName\": \"li\"\n  })), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"p\", {\n    \"parentName\": \"li\"\n  }, \"Stream => Single topic of data irrespective of partitions\")), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"p\", {\n    \"parentName\": \"li\"\n  }, \"[\", mdx(\"a\", {\n    \"href\": \"producer\",\n    \"title\": \"Producer\",\n    \"parentName\": \"p\"\n  }, \"Producer\"), \"]\"), mdx(\"ul\", {\n    \"parentName\": \"li\"\n  }, mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"produce message for a topic, can sometimes specify which partition to store\"))), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"p\", {\n    \"parentName\": \"li\"\n  }, \"[\", mdx(\"a\", {\n    \"href\": \"consumer\",\n    \"title\": \"Consumer\",\n    \"parentName\": \"p\"\n  }, \"Consumer\"), \"]\"), mdx(\"ul\", {\n    \"parentName\": \"li\"\n  }, mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"Reads message\"), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"Subscribes 1:n topics\"), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"FIFO Read (Queue like)\"), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"track of consumed messages via offset\"), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"strong\", {\n    \"parentName\": \"li\"\n  }, \"Ownership\"), \" mapping of consumer to partitions\"))), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"p\", {\n    \"parentName\": \"li\"\n  }, \"Consumer Group -> 1 or more consumers that work together to consumer a topic\"), mdx(\"p\", {\n    \"parentName\": \"li\"\n  })), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"p\", {\n    \"parentName\": \"li\"\n  }, \"Kafka Broker -> Single Kafka server\"), mdx(\"ul\", {\n    \"parentName\": \"li\"\n  }, mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"Receives messages from Producers\"), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"Assign offset\"), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"Commit to disk\"))), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"p\", {\n    \"parentName\": \"li\"\n  }, \"Kafka Cluster\"), mdx(\"ul\", {\n    \"parentName\": \"li\"\n  }, mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"Collection of brokers\"), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"strong\", {\n    \"parentName\": \"li\"\n  }, \"1\"), \" automatically elected broker will function as the cluster \", mdx(\"strong\", {\n    \"parentName\": \"li\"\n  }, \"controller\")), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"Controller responsibilities\", mdx(\"ul\", {\n    \"parentName\": \"li\"\n  }, mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"administrative operations\"), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"assigning partitions to broker\"), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"monitor for broker failures\")))), mdx(\"p\", {\n    \"parentName\": \"li\"\n  })), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"p\", {\n    \"parentName\": \"li\"\n  }, \"Retention -> Configurable for a period of time or size of topic (GB/MB)\")), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"p\", {\n    \"parentName\": \"li\"\n  }, \"Kafka : Pitch\"), mdx(\"ul\", {\n    \"parentName\": \"li\"\n  }, mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"Multiple Producers x Multiple Topics\"), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"Multiple Consumers\"), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"Disk Based Retention --> No data loss\"), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"Scalable\"), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"High Performance\")))), mdx(\"h2\", {\n    \"id\": \"starting-kafka-on-osx\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Starting Kafka on OSX\"), mdx(\"pre\", null, mdx(\"code\", {\n    \"className\": \"language-shell\",\n    \"parentName\": \"pre\"\n  }, \"# Start ZooKeeper\\n# `zkServer` DOESNT WORK IN TMUX!!!\\nzookeeper-server-start /usr/local/etc/kafka/zookeeper.properties & kafka-server-start /usr/local/etc/kafka/server.properties\\n\\n# Start kafka server\\nkafka-server-start.sh /usr/local/etc/kafka/server.properties\\n\\n# Create a topic\\nkafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic temptopic\\n\\n# Start a producer to temptopic\\nkafka-console-producer --broker-list localhost:9092 --topic temptopic\\n\\n# In another tab start a consumer to temptopic\\nkafka-console-producer --broker-list localhost:9092 --topic temptopic --from-beginning\\n\")));\n}\n;\nMDXContent.isMDXComponent = true;","outboundReferences":[{"__typename":"Mdx","body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsx mdx */\nvar _frontmatter = {};\n\nvar makeShortcode = function makeShortcode(name) {\n  return function MDXDefaultShortcode(props) {\n    console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\");\n    return mdx(\"div\", props);\n  };\n};\n\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"producer\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Producer\"), mdx(\"p\", null, \"Kafka has a binary wire protocol.This means that it is possible for applications to read messages from Kafka or write messages to Kafka simply by sending the correct byte sequences to Kafka\\u2019s network port.\"), mdx(\"p\", null, \"Key Structure is \", mdx(\"inlineCode\", {\n    \"parentName\": \"p\"\n  }, \"ProducerRecord\")), mdx(\"p\", null), mdx(\"h1\", {\n    \"id\": \"serializer\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Serializer\"), mdx(\"p\", null, \"Converts between wire format and code\"), mdx(\"p\", null, mdx(\"a\", {\n    \"href\": \"https://https://avro.apache.org/docs/current\",\n    \"parentName\": \"p\"\n  }, \"Apache Avro\"), \"\\nDefine a common schema for serialization and deserialization\\nStore in schema Registry\\nStore schema identifier in produced message\\n\"), mdx(\"h1\", {\n    \"id\": \"creating-a-producer\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Creating a Producer\"), mdx(\"h2\", {\n    \"id\": \"in-rust\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"In [\", mdx(\"a\", {\n    \"href\": \"rust\",\n    \"title\": \"Rust\",\n    \"parentName\": \"h2\"\n  }, \"rust\"), \"]\"), mdx(\"p\", null, \"Crate : \", mdx(\"a\", {\n    \"href\": \"https://crates.io/crates/kafka\",\n    \"parentName\": \"p\"\n  }, \"kafka\")), mdx(\"pre\", null, mdx(\"code\", {\n    \"className\": \"language-rust\",\n    \"parentName\": \"pre\"\n  }, \"/// Producer Code\\nuse std::fmt::Write;\\nuse std::time::Duration;\\nuse kafka::producer::{Producer, Record, RequiredAcks};\\n\\nlet mut producer = Producer::from_hosts(vec!(\\\"localhost:9092\\\".to_owned()))\\n.with_ack_timeout(Duration::from_secs(1))\\n.with_required_acks(RequiredAcks::One)\\n.create()\\n.unwrap();\\n\\nlet mut buf = String::with*capacity(2);\\nfor i in 0..10 {\\nlet * = write!(&mut buf, \\\"{}\\\", i); // some computation of the message data to be sent\\nproducer.send(&Record::from_value(\\\"my-topic\\\", buf.as_bytes())).unwrap();\\nbuf.clear();\\n}\\n\")), mdx(\"p\", null, \"Crate : \", mdx(\"a\", {\n    \"href\": \"https://crates.io/crates/rdkafka\",\n    \"parentName\": \"p\"\n  }, \"rdkafka\")), mdx(\"pre\", null, mdx(\"code\", {\n    \"className\": \"language-Rust\",\n    \"parentName\": \"pre\"\n  }, \" sync fn produce(brokers: &str, topic_name: &str) {\\n    let producer: &FutureProducer = &ClientConfig::new()\\n        .set(\\\"bootstrap.servers\\\", brokers)\\n        .set(\\\"message.timeout.ms\\\", \\\"5000\\\")\\n        .create()\\n        .expect(\\\"Producer creation error\\\");\\n\\n    // This loop is non blocking: all messages will be sent one after the other, without waiting\\n    // for the results.\\n    let futures = (0..5)\\n        .map(|i| async move {\\n            // The send operation on the topic returns a future, which will be\\n            // completed once the result or failure from Kafka is received.\\n            let delivery_status = producer\\n                .send(\\n                    FutureRecord::to(topic_name)\\n                        .payload(&format!(\\\"Message {}\\\", i))\\n                        .key(&format!(\\\"Key {}\\\", i))\\n                        .headers(OwnedHeaders::new().add(\\\"header_key\\\", \\\"header_value\\\")),\\n                    Duration::from_secs(0),\\n                )\\n                .await;\\n\\n            // This will be executed when the result is received.\\n            info!(\\\"Delivery status for message {} received\\\", i);\\n            delivery_status\\n        })\\n        .collect::<Vec<_>>();\\n\\n    // This loop will wait until all delivery statuses have been received.\\n    for future in futures {\\n        info!(\\\"Future completed. Result: {:?}\\\", future.await);\\n    }\\n}\\n\\n\")), mdx(\"p\", null, \"Schema Registry for confluent : \", mdx(\"a\", {\n    \"href\": \"https://github.com/gklijs/schema_registry_converter\",\n    \"parentName\": \"p\"\n  }, \"https://github.com/gklijs/schema_registry_converter\")), mdx(\"p\", null, \"Confluent Write up : \", mdx(\"a\", {\n    \"href\": \"https://www.confluent.io/blog/getting-started-with-rust-and-kafka/\",\n    \"parentName\": \"p\"\n  }, \"https://www.confluent.io/blog/getting-started-with-rust-and-kafka/\")), mdx(\"hr\", null), mdx(\"h2\", {\n    \"id\": \"in-golang\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"In [\", mdx(\"a\", {\n    \"href\": \"golang\",\n    \"title\": \"Golang\",\n    \"parentName\": \"h2\"\n  }, \"golang\"), \"]\"), mdx(\"p\", null, \"Package : \", mdx(\"a\", {\n    \"href\": \"https://github.com/confluentinc/confluent-kafka-go\",\n    \"parentName\": \"p\"\n  }, \"Confluent Inc\")), mdx(\"pre\", null, mdx(\"code\", {\n    \"className\": \"language-golang\",\n    \"parentName\": \"pre\"\n  }, \"func main() {\\n\\n    p, err := kafka.NewProducer(&kafka.ConfigMap{\\\"bootstrap.servers\\\": \\\"localhost\\\"})\\n    if err != nil {\\n        panic(err)\\n    }\\n\\n    defer p.Close()\\n\\n    // Delivery report handler for produced messages\\n    go func() {\\n        for e := range p.Events() {\\n            switch ev := e.(type) {\\n            case *kafka.Message:\\n                if ev.TopicPartition.Error != nil {\\n                    fmt.Printf(\\\"Delivery failed: %v\\\\n\\\", ev.TopicPartition)\\n                } else {\\n                    fmt.Printf(\\\"Delivered message to %v\\\\n\\\", ev.TopicPartition)\\n                }\\n            }\\n        }\\n    }()\\n\\n    // Produce messages to topic (asynchronously)\\n    topic := \\\"myTopic\\\"\\n    for _, word := range []string{\\\"Welcome\\\", \\\"to\\\", \\\"the\\\", \\\"Confluent\\\", \\\"Kafka\\\", \\\"Golang\\\", \\\"client\\\"} {\\n        p.Produce(&kafka.Message{\\n            TopicPartition: kafka.TopicPartition{Topic: &topic, Partition: kafka.PartitionAny},\\n            Value:          []byte(word),\\n        }, nil)\\n    }\\n\\n    // Wait for message deliveries before shutting down\\n    p.Flush(15 * 1000)\\n}\\n\")), mdx(\"p\", null, \"Package : \", mdx(\"a\", {\n    \"href\": \"https://github.com/segmentio/kafka-go\",\n    \"parentName\": \"p\"\n  }, \"SegmentIO\")), mdx(\"pre\", null, mdx(\"code\", {\n    \"className\": \"language-golang\",\n    \"parentName\": \"pre\"\n  }, \"func producer() {\\n\\n    topic := \\\"my-topic\\\"\\n    partition := 0\\n\\n    conn, _ := kafka.DialLeader(context.Background(), \\\"tcp\\\", \\\"localhost:9092\\\", topic, partition)\\n\\n    conn.SetWriteDeadline(time.Now().Add(10*time.Second))\\n    conn.WriteMessages(\\n    kafka.Message{Value: []byte(\\\"one!\\\")},\\n    kafka.Message{Value: []byte(\\\"two!\\\")},\\n    kafka.Message{Value: []byte(\\\"three!\\\")},\\n    )\\n\\n    conn.Close()\\n}\\n\")), mdx(\"p\", null, \"Package : \", mdx(\"a\", {\n    \"href\": \"https://github.com/Shopify/sarama\",\n    \"parentName\": \"p\"\n  }, \"Sarama\")), mdx(\"pre\", null, mdx(\"code\", {\n    \"className\": \"language-golang\",\n    \"parentName\": \"pre\"\n  }, \"//Sync Producer\\nfunc newDataCollector(brokerList []string) sarama.SyncProducer {\\n\\n    // For the data collector, we are looking for strong consistency semantics.\\n    // Because we don't change the flush settings, sarama will try to produce messages\\n    // as fast as possible to keep latency low.\\n    config := sarama.NewConfig()\\n    config.Producer.RequiredAcks = sarama.WaitForAll // Wait for all in-sync replicas to ack the message\\n    config.Producer.Retry.Max = 10                   // Retry up to 10 times to produce the message\\n    config.Producer.Return.Successes = true\\n    tlsConfig := createTlsConfiguration()\\n    if tlsConfig != nil {\\n        config.Net.TLS.Config = tlsConfig\\n        config.Net.TLS.Enable = true\\n    }\\n\\n    // On the broker side, you may want to change the following settings to get\\n    // stronger consistency guarantees:\\n    // - For your broker, set `unclean.leader.election.enable` to false\\n    // - For the topic, you could increase `min.insync.replicas`.\\n\\n    producer, err := sarama.NewSyncProducer(brokerList, config)\\n    if err != nil {\\n        log.Fatalln(\\\"Failed to start Sarama producer:\\\", err)\\n    }\\n\\n    return producer\\n}\\n\\n// Async Producer\\nfunc newAccessLogProducer(brokerList []string) sarama.AsyncProducer {\\n\\n    // For the access log, we are looking for AP semantics, with high throughput.\\n    // By creating batches of compressed messages, we reduce network I/O at a cost of more latency.\\n    config := sarama.NewConfig()\\n    tlsConfig := createTlsConfiguration()\\n    if tlsConfig != nil {\\n        config.Net.TLS.Enable = true\\n        config.Net.TLS.Config = tlsConfig\\n    }\\n    config.Producer.RequiredAcks = sarama.WaitForLocal       // Only wait for the leader to ack\\n    config.Producer.Compression = sarama.CompressionSnappy   // Compress messages\\n    config.Producer.Flush.Frequency = 500 * time.Millisecond // Flush batches every 500ms\\n\\n    producer, err := sarama.NewAsyncProducer(brokerList, config)\\n    if err != nil {\\n        log.Fatalln(\\\"Failed to start Sarama producer:\\\", err)\\n    }\\n\\n    // We will just log to STDOUT if we're not able to produce messages.\\n    // Note: messages will only be returned here after all retry attempts are exhausted.\\n    go func() {\\n        for err := range producer.Errors() {\\n            log.Println(\\\"Failed to write access log entry:\\\", err)\\n        }\\n    }()\\n\\n    return producer\\n}\\n\\ngo func sendMessage(){\\n        // Synch\\n        partition, offset, err := s.DataCollector.SendMessage(&sarama.ProducerMessage{\\n            Topic: \\\"important\\\",\\n            Value: sarama.StringEncoder(r.URL.RawQuery),\\n        })\\n        // Async\\n        s.AccessLogProducer.Input() <- &sarama.ProducerMessage{\\n            Topic: \\\"access_log\\\",\\n            Key:   sarama.StringEncoder(r.RemoteAddr),\\n            Value: entry,\\n        }\\n}\\n\")));\n}\n;\nMDXContent.isMDXComponent = true;","parent":{"__typename":"File","id":"67abd20c-6607-580b-bb55-74eeb8467444","fields":{"slug":"/producer","title":"Producer"}}},{"__typename":"Mdx","body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsx mdx */\nvar _frontmatter = {};\n\nvar makeShortcode = function makeShortcode(name) {\n  return function MDXDefaultShortcode(props) {\n    console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\");\n    return mdx(\"div\", props);\n  };\n};\n\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"consumer\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Consumer\"), mdx(\"ul\", null, mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"p\", {\n    \"parentName\": \"li\"\n  }, \"Subscribes to topics and receives message\")), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"p\", {\n    \"parentName\": \"li\"\n  }, \"Consumer Group -> method of scaling consumption\"), mdx(\"ul\", {\n    \"parentName\": \"li\"\n  }, mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"Membership triggered by a \", mdx(\"inlineCode\", {\n    \"parentName\": \"li\"\n  }, \"JoinGroup\"), \" call\"), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"First member is the de-facto group leader\"), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"leader assigns partitions to consumers in the consumer group\"), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"Membership is maintained by sending \", mdx(\"em\", {\n    \"parentName\": \"li\"\n  }, \"heartbeats\"), \" at intervals regularly to the \", mdx(\"em\", {\n    \"parentName\": \"li\"\n  }, \"group coordinator\")), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"Sent during \", mdx(\"inlineCode\", {\n    \"parentName\": \"li\"\n  }, \"poll\"), \" and \", mdx(\"inlineCode\", {\n    \"parentName\": \"li\"\n  }, \"commit\")), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"missing heartbeats for a period will trigger rebalance as the consumer is thought to be dead\"))), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"p\", {\n    \"parentName\": \"li\"\n  }, mdx(\"em\", {\n    \"parentName\": \"p\"\n  }, \"Rebalance\"), \" moving partition ownership from one consumer to another\"), mdx(\"ul\", {\n    \"parentName\": \"li\"\n  }, mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"high scalability and availability\"), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"creates a short unavailability window\")))), mdx(\"p\", null), mdx(\"h1\", {\n    \"id\": \"creating-a-consumer\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Creating a Consumer\"), mdx(\"h2\", {\n    \"id\": \"in-rust\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"In [\", mdx(\"a\", {\n    \"href\": \"rust\",\n    \"title\": \"Rust\",\n    \"parentName\": \"h2\"\n  }, \"rust\"), \"]\"), mdx(\"p\", null, \"Crate : \", mdx(\"a\", {\n    \"href\": \"https://crates.io/crates/kafka\",\n    \"parentName\": \"p\"\n  }, \"kafka\")), mdx(\"pre\", null, mdx(\"code\", {\n    \"className\": \"language-Rust\",\n    \"parentName\": \"pre\"\n  }, \"use kafka::consumer::{Consumer, FetchOffset, GroupOffsetStorage};\\nlet mut consumer =\\n   Consumer::from_hosts(vec!(\\\"localhost:9092\\\".to_owned()))\\n      .with_topic_partitions(\\\"my-topic\\\".to_owned(), &[0, 1])\\n      .with_fallback_offset(FetchOffset::Earliest)\\n      .with_group(\\\"my-group\\\".to_owned())\\n      .with_offset_storage(GroupOffsetStorage::Kafka)\\n      .create()\\n      .unwrap();\\nloop {\\n  for ms in consumer.poll().unwrap().iter() {\\n    for m in ms.messages() {\\n      println!(\\\"{:?}\\\", m);\\n    }\\n    consumer.consume_messageset(ms);\\n  }\\n  consumer.commit_consumed().unwrap();\\n}\\n\")), mdx(\"p\", null, \"Crate : \", mdx(\"a\", {\n    \"href\": \"https://crates.io/crates/rdkafka\",\n    \"parentName\": \"p\"\n  }, \"rdkafka\")), mdx(\"pre\", null, mdx(\"code\", {\n    \"className\": \"language-Rust\",\n    \"parentName\": \"pre\"\n  }, \"async fn consume(brokers: &str, group_id: &str, topics: &[&str]) {\\n    let context = CustomContext;\\n\\n    let consumer: LoggingConsumer = ClientConfig::new()\\n        .set(\\\"group.id\\\", group_id)\\n        .set(\\\"bootstrap.servers\\\", brokers)\\n        .set(\\\"enable.partition.eof\\\", \\\"false\\\")\\n        .set(\\\"session.timeout.ms\\\", \\\"6000\\\")\\n        .set(\\\"enable.auto.commit\\\", \\\"true\\\")\\n        //.set(\\\"statistics.interval.ms\\\", \\\"30000\\\")\\n        //.set(\\\"auto.offset.reset\\\", \\\"smallest\\\")\\n        .set_log_level(RDKafkaLogLevel::Debug)\\n        .create_with_context(context)\\n        .expect(\\\"Consumer creation failed\\\");\\n\\n    consumer\\n        .subscribe(&topics.to_vec())\\n        .expect(\\\"Can't subscribe to specified topics\\\");\\n\\n    // consumer.start() returns a stream. The stream can be used ot chain together expensive steps,\\n    // such as complex computations on a thread pool or asynchronous IO.\\n    let mut message_stream = consumer.start();\\n\\n    while let Some(message) = message_stream.next().await {\\n        match message {\\n            Err(e) => warn!(\\\"Kafka error: {}\\\", e),\\n            Ok(m) => {\\n                let payload = match m.payload_view::<str>() {\\n                    None => \\\"\\\",\\n                    Some(Ok(s)) => s,\\n                    Some(Err(e)) => {\\n                        warn!(\\\"Error while deserializing message payload: {:?}\\\", e);\\n                        \\\"\\\"\\n                    }\\n                };\\n                info!(\\\"key: '{:?}', payload: '{}', topic: {}, partition: {}, offset: {}, timestamp: {:?}\\\",\\n                      m.key(), payload, m.topic(), m.partition(), m.offset(), m.timestamp());\\n                if let Some(headers) = m.headers() {\\n                    for i in 0..headers.count() {\\n                        let header = headers.get(i).unwrap();\\n                        info!(\\\"  Header {:#?}: {:?}\\\", header.0, header.1);\\n                    }\\n                }\\n                consumer.commit_message(&m, CommitMode::Async).unwrap();\\n            }\\n        };\\n    }\\n}\\n\")), mdx(\"hr\", null), mdx(\"h2\", {\n    \"id\": \"in-golang\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"In [\", mdx(\"a\", {\n    \"href\": \"golang\",\n    \"title\": \"Golang\",\n    \"parentName\": \"h2\"\n  }, \"golang\"), \"]\"), mdx(\"p\", null, \"Package : \", mdx(\"a\", {\n    \"href\": \"https://github.com/confluentinc/confluent-kafka-go\",\n    \"parentName\": \"p\"\n  }, \"Confluent Inc\")), mdx(\"pre\", null, mdx(\"code\", {\n    \"className\": \"language-golang\",\n    \"parentName\": \"pre\"\n  }, \"func main() {\\n\\n    c, err := kafka.NewConsumer(&kafka.ConfigMap{\\n        \\\"bootstrap.servers\\\": \\\"localhost\\\",\\n        \\\"group.id\\\":          \\\"myGroup\\\",\\n        \\\"auto.offset.reset\\\": \\\"earliest\\\",\\n    })\\n\\n    if err != nil {\\n        panic(err)\\n    }\\n\\n    c.SubscribeTopics([]string{\\\"myTopic\\\", \\\"^aRegex.*[Tt]opic\\\"}, nil)\\n\\n    for {\\n        msg, err := c.ReadMessage(-1)\\n        if err == nil {\\n            fmt.Printf(\\\"Message on %s: %s\\\\n\\\", msg.TopicPartition, string(msg.Value))\\n        } else {\\n            // The client will automatically try to recover from all errors.\\n            fmt.Printf(\\\"Consumer error: %v (%v)\\\\n\\\", err, msg)\\n        }\\n    }\\n\\n    c.Close()\\n}\\n\")), mdx(\"h2\", {\n    \"id\": \"package--segmentio\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Package : \", mdx(\"a\", {\n    \"href\": \"https://github.com/segmentio/kafka-go\",\n    \"parentName\": \"h2\"\n  }, \"SegmentIO\")), mdx(\"pre\", null, mdx(\"code\", {\n    \"className\": \"language-golang\",\n    \"parentName\": \"pre\"\n  }, \"func consumer(){}\\nr := kafka.NewReader(kafka.ReaderConfig{\\n    Brokers:   []string{\\\"localhost:9092\\\"},\\n    Topic:     \\\"topic-A\\\",\\n    Partition: 0,\\n    MinBytes:  10e3, // 10KB\\n    MaxBytes:  10e6, // 10MB\\n})\\nr.SetOffset(42)\\n\\nfor {\\n    m, err := r.ReadMessage(context.Background())\\n    if err != nil {\\n        break\\n    }\\n    fmt.Printf(\\\"message at offset %d: %s = %s\\\\n\\\", m.Offset, string(m.Key), string(m.Value))\\n}\\n\\nr.Close()\\n}\\n\")), mdx(\"p\", null, \"Package : \", mdx(\"a\", {\n    \"href\": \"https://github.com/Shopify/sarama\",\n    \"parentName\": \"p\"\n  }, \"Sarama\")), mdx(\"pre\", null, mdx(\"code\", {\n    \"className\": \"language-golang\",\n    \"parentName\": \"pre\"\n  }, \"// ConsumeClaim must start a consumer loop of ConsumerGroupClaim's Messages().\\nfunc (consumer *Consumer) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error {\\n\\n    // NOTE:\\n    // Do not move the code below to a goroutine.\\n    // The `ConsumeClaim` itself is called within a goroutine, see:\\n    // https://github.com/Shopify/sarama/blob/master/consumer_group.go#L27-L29\\n    for message := range claim.Messages() {\\n        log.Printf(\\\"Message claimed: value = %s, timestamp = %v, topic = %s\\\", string(message.Value), message.Timestamp, message.Topic)\\n        session.MarkMessage(message, \\\"\\\")\\n    }\\n\\n    return nil\\n}\\n\\ngo func() comsumer{\\n        defer wg.Done()\\n        for {\\n            // `Consume` should be called inside an infinite loop, when a\\n            // server-side rebalance happens, the consumer session will need to be\\n            // recreated to get the new claims\\n            if err := client.Consume(ctx, strings.Split(topics, \\\",\\\"), &consumer); err != nil {\\n                log.Panicf(\\\"Error from consumer: %v\\\", err)\\n            }\\n            // check if context was cancelled, signaling that the consumer should stop\\n            if ctx.Err() != nil {\\n                return\\n            }\\n            consumer.ready = make(chan bool)\\n        }\\n    }()\\n\\n\")), mdx(\"h1\", {\n    \"id\": \"commits\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Commits\"), mdx(\"ul\", null, mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"p\", {\n    \"parentName\": \"li\"\n  }, \"Consumers use kafka to track their position in each partition\")), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"p\", {\n    \"parentName\": \"li\"\n  }, mdx(\"inlineCode\", {\n    \"parentName\": \"p\"\n  }, \"commit\"), \" act of updating current position in kafka\")), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"p\", {\n    \"parentName\": \"li\"\n  }, \"message \", mdx(\"inlineCode\", {\n    \"parentName\": \"p\"\n  }, \"__consumer_offsets\"), \" topic with the offset for each partition\")), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"p\", {\n    \"parentName\": \"li\"\n  }, \"when rebalance each consumer receives a new partition and obtains the latest committed offset from where to start reading\")), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"p\", {\n    \"parentName\": \"li\"\n  }, \"Commit Strategy\"), mdx(\"ul\", {\n    \"parentName\": \"li\"\n  }, mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"p\", {\n    \"parentName\": \"li\"\n  }, \"Automatic\"), mdx(\"ul\", {\n    \"parentName\": \"li\"\n  }, mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"Done by consumer every 5 seconds configurable\"), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"Commits largest offset from \", mdx(\"inlineCode\", {\n    \"parentName\": \"li\"\n  }, \"poll\")), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"call to \", mdx(\"inlineCode\", {\n    \"parentName\": \"li\"\n  }, \"poll\"), \" will always commit the last offset\"), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"results in double processing if \", mdx(\"inlineCode\", {\n    \"parentName\": \"li\"\n  }, \"rebalance\"), \" occurs in between a 5 sec window\"))), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"p\", {\n    \"parentName\": \"li\"\n  }, \"Commit Current Offset\"), mdx(\"ul\", {\n    \"parentName\": \"li\"\n  }, mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"Gives control to the developer\"), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"inlineCode\", {\n    \"parentName\": \"li\"\n  }, \"commitSync\"), \" triggers commit of the last offset returned by \", mdx(\"inlineCode\", {\n    \"parentName\": \"li\"\n  }, \"poll\")), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"must be called after processing all messages\"), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"Synchronous call blocks the application\"), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"Auto retry till success of non retry-able failure\"))), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"p\", {\n    \"parentName\": \"li\"\n  }, \"Asynchronous Commit\"), mdx(\"ul\", {\n    \"parentName\": \"li\"\n  }, mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"Fire and forget till we get a callback\"), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"Does not auto retry as a later commit request might have latest Offset\"))), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"p\", {\n    \"parentName\": \"li\"\n  }, \"Async + Sync Commit\"), mdx(\"ul\", {\n    \"parentName\": \"li\"\n  }, mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, mdx(\"inlineCode\", {\n    \"parentName\": \"li\"\n  }, \"CommitAsync\"), \" always\"), mdx(\"li\", {\n    \"parentName\": \"ul\"\n  }, \"Trigger a CommitSync just before exit\")))))));\n}\n;\nMDXContent.isMDXComponent = true;","parent":{"__typename":"File","id":"a9f33b82-4aa7-599b-a242-54db28ac76f1","fields":{"slug":"/consumer","title":"Consumer"}}}],"inboundReferences":[{"__typename":"Mdx","body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsx mdx */\nvar _frontmatter = {};\n\nvar makeShortcode = function makeShortcode(name) {\n  return function MDXDefaultShortcode(props) {\n    console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\");\n    return mdx(\"div\", props);\n  };\n};\n\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"foam-bubbles\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Foam Bubbles\"), mdx(\"p\", null, \"[\", mdx(\"a\", {\n    \"href\": \"inbox\",\n    \"title\": \"Inbox\",\n    \"parentName\": \"p\"\n  }, \"inbox\"), \"]\"), mdx(\"p\", null, \"[\", mdx(\"a\", {\n    \"href\": \"todo\",\n    \"title\": \"Todo / Reading List\",\n    \"parentName\": \"p\"\n  }, \"todo\"), \"]\"), mdx(\"p\", null, \"[\", mdx(\"a\", {\n    \"href\": \"dwarf\",\n    \"title\": \"Dwarf Debugging Format\",\n    \"parentName\": \"p\"\n  }, \"dwarf\"), \"]\"), mdx(\"p\", null, \"[\", mdx(\"a\", {\n    \"href\": \"kafka\",\n    \"title\": \"Apache Kafka\",\n    \"parentName\": \"p\"\n  }, \"kafka\"), \"]\"), mdx(\"p\", null, \"[\", mdx(\"a\", {\n    \"href\": \"elf\",\n    \"title\": \"elf\",\n    \"parentName\": \"p\"\n  }, \"elf\"), \"]\"), mdx(\"p\", null, \"[\", mdx(\"a\", {\n    \"href\": \"rust\",\n    \"title\": \"Rust\",\n    \"parentName\": \"p\"\n  }, \"rust\"), \"]\"), mdx(\"p\", null, \"[\", mdx(\"a\", {\n    \"href\": \"golang\",\n    \"title\": \"Golang\",\n    \"parentName\": \"p\"\n  }, \"golang\"), \"]\"));\n}\n;\nMDXContent.isMDXComponent = true;","parent":{"__typename":"File","id":"0b774b8d-a51d-55d2-8d2d-8f7d194fe0d5","fields":{"slug":"/readme","title":"Foam Bubbles"}}}]},"fields":{"slug":"/kafka","title":"Apache Kafka"}}},"pageContext":{"id":"1e7fcadb-7efc-56d0-a835-534146fc3704"}},"staticQueryHashes":["2098632890","426988268"]}